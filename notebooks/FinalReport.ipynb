{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e7abff-98e8-49d8-b374-047304fa6952",
   "metadata": {},
   "source": [
    "# Electricity Load Forecasting Using Machine Learning  \n",
    "**CS 4347 BAK – Project Report**  \n",
    "**Team: Aryal, Bhetuwal, Khulal**  \n",
    "**Semester: Fall 2025**\n",
    "\n",
    "---\n",
    "\n",
    "## Team Members & Roles\n",
    "| Name | NetID | Roles |\n",
    "|------|--------|--------|\n",
    "| **Sebika Khulal** | zbb20 | Exploratory Data Analysis (EDA), Data Cleaning, Baseline Linear Regression |\n",
    "| **Ananta Aryal** | ldi23 | Lag-Based Feature Engineering, Time-Series Split, Preprocessing Support |\n",
    "| **Anubhav Bhetuwal** | qrf8 | Normalization Analysis, Sparse Random Projection (SRP), Ridge/Lasso Tuning, XGBoost Modeling |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe850b7-1199-439c-89e2-17a9b62e8ad9",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This project investigates forecasting household electricity load using a high-dimensional smart-meter dataset from **OpenML**.  \n",
    "The dataset includes **105,217 observations** and **316 continuous meter-reading features** (value_0–value_315), totaling more than **33 million numeric entries**.\n",
    "\n",
    "Exploratory Data Analysis (EDA) shows that the data is **highly skewed**, **sparse**, and affected by **large scale differences** across meters, with **weak linear correlations** to the target.  \n",
    "These characteristics make simple unregularized linear models underfit and highlight the need for methods that handle noise, redundancy, and imbalance effectively.\n",
    "\n",
    "We began with a **baseline Linear Regression** model, then introduced improved methods including **Ridge**, **Lasso**, and **XGBoost**.  \n",
    "To address the extreme dimensionality, we applied **Sparse Random Projection (SRP)**, following the professor’s guidance on dimensionality-reduction methods for high-dimensional data.\n",
    "\n",
    "Our best-performing model, **Lasso Regression**, achieved a **test RMSE of 6.35**, outperforming Ridge and SRP + XGBoost.  \n",
    "This shows that regularization and feature selection provide the strongest performance for this dataset.\n",
    "\n",
    "Following the feedback of the profressor, we expanded our preprocessing stage to include a detailed normalization analysis, ensuring that scale imbalance and skewness across meter readings were properly addressed before model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660b751-d82f-4239-a4de-7c2112913778",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The objective of this project is to **predict `value_0`**, an electricity load measurement, using the remaining **315 smart-meter features** as predictors.  \n",
    "The dataset spans multiple households over several years and is both **high-dimensional** and **temporally structured**.\n",
    "\n",
    "Our initial plan was to apply PCA for dimensionality reduction. However, based on professor feedback, we transitioned to **Sparse Random Projection (SRP)**, which is better suited for preserving pairwise distances in high-dimensional data.\n",
    "\n",
    "Our understanding of the problem evolved substantially after conducting EDA:\n",
    "\n",
    "- Meter readings have **large differences in scale**.  \n",
    "- Many features show **long stretches of zeros**, indicating sparsity.  \n",
    "- Pairwise correlations with the target are **very weak** (all < 0.22).  \n",
    "- Scatterplots reveal **banding patterns and irregular structure**, indicating noise and weak linear relationships.\n",
    "\n",
    "These observations show that simple unregularized linear models will not perform well.  \n",
    "The task therefore benefits from **regularization** and **dimensionality reduction**, while nonlinear models remain optional rather than strictly required.\n",
    "\n",
    "After performing EDA, we realized that the dataset was far more sparse, skewed, and scale-imbalanced than we initially assumed. This shifted our plan away from PCA toward Sparse Random Projection and regularized models that are better suited to noisy, high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3074ae-a019-4b6b-a351-d9506a6539fe",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "- **Source:** OpenML Dataset #46214, Electricity Load Diagrams (2011–2014)\n",
    "- **Rows:** 105,217\n",
    "- **Features:** 319  \n",
    "  - value_0–value_315 (continuous meter readings)  \n",
    "  - id_series (meter ID)  \n",
    "  - date (timestamp)  \n",
    "  - time_step (time index)\n",
    "\n",
    "### Dataset Challenges:\n",
    "- No missing numeric values, but strong sparsity patterns.\n",
    "- Extremely skewed distributions.\n",
    "- Features vary from near-zero to thousands.\n",
    "- Temporal structure must be respected (no shuffling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d67bc-7a2e-42cc-a5dc-b9fefc15d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"../data/electricity.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1a8ce-a4b3-422a-aa29-5d6996a822d9",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Below we summarize the structure, distribution, and relationships in the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acd964-c8b4-410a-87b8-f63305bb1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb67997-2ea9-48bf-a938-3cc920eea561",
   "metadata": {},
   "source": [
    "## Summary Interpretation\n",
    "\n",
    "- Most features have medians near 0 → sparse consumption.\n",
    "- Many features have extreme max values → outliers present.\n",
    "- Standard deviations vary widely → normalization required.\n",
    "\n",
    "These differences in scale motivate the use of StandardScaler for all regularized linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2bbf8e-078c-432f-9a5f-9387594e12e0",
   "metadata": {},
   "source": [
    "### Distribution of Key Features\n",
    "\n",
    "To visualize the skewness and long-tailed nature of the meter readings observed in the summary statistics, we plot histograms for a few representative features. These plots confirm that the dataset contains many zeros, strong right-skew, and occasional large spikes in consumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87218a0-dfa7-4fac-a4f6-be8e03d74554",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df[\"value_0\"], bins=50, kde=True, color=\"steelblue\")\n",
    "plt.title(\"Distribution of value_0\")\n",
    "plt.xlabel(\"value_0\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df[\"value_47\"], bins=50, kde=True, color=\"darkorange\")\n",
    "plt.title(\"Distribution of value_47\")\n",
    "plt.xlabel(\"value_47\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df[\"value_116\"], bins=50, kde=True, color=\"forestgreen\")\n",
    "plt.title(\"Distribution of value_116\")\n",
    "plt.xlabel(\"value_116\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a26ce-bcef-4061-9c7e-02a3bd307471",
   "metadata": {},
   "source": [
    "### Interpretation of Histograms\n",
    "\n",
    "- All three features show strong right-skew.\n",
    "- Large clusters at zero reflect periods of no consumption.\n",
    "- Occasional spikes indicate unusual high-usage events.\n",
    "- These patterns reinforce the need for **normalization** and for models that can handle noise, scale differences, and irregular consumption patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2edf56-9d80-4d9d-8de0-d3ba77935ab7",
   "metadata": {},
   "source": [
    "### How EDA Guided Our Model Choices\n",
    "\n",
    "The EDA showed that most features are sparse with long right tails, that standard deviations vary widely across meters, and that the strongest absolute correlation with the target is below 0.22. These findings suggest that (1) unregularized linear regression will underfit because no single feature is strongly predictive, and (2) models are sensitive to scale and outliers. As a result, we focused on regularized linear models (Ridge and Lasso) to handle high-dimensional noise, and on Sparse Random Projection combined with XGBoost to explore nonlinear interactions while controlling dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92296e7b-760b-4c78-9279-c8e81a046146",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"value_0\"\n",
    "\n",
    "corr = (\n",
    "    df.corr(numeric_only=True)[target]\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=corr.values, y=corr.index)\n",
    "plt.title(\"Top 15 Features Correlated with value_0\")\n",
    "plt.xlabel(\"Absolute Correlation\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e9ba6-eccd-41c5-8728-2ee20eef3ece",
   "metadata": {},
   "source": [
    "## Interpretation of Correlation Barplot\n",
    "\n",
    "- No feature has correlation > 0.22 with `value_0`.  \n",
    "- This confirms **weak linear relationships** across meters.  \n",
    "- Plain Linear Regression is expected to **underfit** because no single feature provides strong signal.  \n",
    "- This motivates the use of **regularization** (Ridge/Lasso) and **dimensionality reduction**, while nonlinear models may help but are not strictly required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1f9c9-9cf4-4452-957d-f6a68f91e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = corr.index[1:4]  # skip value_0 itself\n",
    "\n",
    "for f in top_features:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df[f].sample(3000), df[target].sample(3000), alpha=0.3)\n",
    "    plt.title(f\"{f} vs value_0\")\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(\"value_0\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a5e0f-7975-4e56-9b6d-fb9a6625ed57",
   "metadata": {},
   "source": [
    "## Scatterplot Interpretation\n",
    "\n",
    "- Strong nonlinear structure.\n",
    "- Distinct horizontal “banding” patterns from discrete meter behaviors.\n",
    "- No clear linear trend linking features to value_0.\n",
    "- This confirms that linear models without regularization will struggle, but does not guarantee that nonlinear models outperform regularized linear ones.\n",
    "- Banding appears because many meters report consumption in fixed increments, producing horizontal plateaus unrelated to value_0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dc479-2012-4dec-89e8-a1db15842c03",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Why Normalization?\n",
    "\n",
    "- Features differ drastically in scale (0 to >3000).\n",
    "- Helps stabilize training for Ridge, Lasso, and XGBoost.\n",
    "- Prevents large magnitude features from dominating.\n",
    "\n",
    "## Method\n",
    "We use **StandardScaler** applied on the training split only (to avoid data leakage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c4476-a34a-43e6-b848-513781264def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "sample_raw = df[top_features].head()\n",
    "sample_scaled = scaler.fit_transform(sample_raw)\n",
    "\n",
    "pd.DataFrame(sample_scaled, columns=top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb390e-2422-48ac-ab45-e347ffff84a3",
   "metadata": {},
   "source": [
    "### Normalization Effects: Before vs After\n",
    "\n",
    "To verify the impact of StandardScaler on the data distribution, we compared the raw and scaled versions of three representative features: the target `value_0`, a medium-scale feature `value_47`, and a large-scale feature `value_116`. These features were chosen because they span very different ranges and appear prominently in the EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe5eab-e150-42c7-8abc-f0c994bf9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = [\"value_0\", \"value_47\", \"value_116\"]\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(df[features])\n",
    "scaled_df = pd.DataFrame(scaled, columns=[f + \"_scaled\" for f in features])\n",
    "\n",
    "combined = pd.concat([df[features], scaled_df], axis=1)\n",
    "\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d29b04-dd41-4fc2-bc71-a97a420546db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw vs scaled histograms for representative features\n",
    "features = [\"value_0\", \"value_47\", \"value_116\"]\n",
    "\n",
    "for f in features:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Raw\n",
    "    sns.histplot(df[f], bins=50, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(f\"Raw Distribution of {f}\")\n",
    "    axes[0].set_xlabel(f)\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    # Scaled\n",
    "    sns.histplot(combined[f + \"_scaled\"], bins=50, kde=True, color=\"orange\", ax=axes[1])\n",
    "    axes[1].set_title(f\"Scaled Distribution of {f}\")\n",
    "    axes[1].set_xlabel(f\"{f} (scaled)\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f66701-d2de-4c41-b7df-fa87a9f9de9e",
   "metadata": {},
   "source": [
    "The histograms show that all three features are heavily right-skewed in the raw space, with long tails and very different scales. After normalization, the transformed features are centered around zero with comparable spread, which prevents any single meter from dominating the loss during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93585e2e-6eee-4c28-97ee-6880d4f6e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side boxplots for raw vs scaled features\n",
    "for f in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=combined[[f, f + \"_scaled\"]])\n",
    "    plt.title(f\"Before vs After Normalization: {f}\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3440e-bd2b-40f6-be90-2bc38766f48c",
   "metadata": {},
   "source": [
    "The side-by-side boxplots illustrate how normalization compresses the range and rescales extreme values, especially for `value_47` and `value_116`. While outliers are still present, they are now expressed on a comparable standardized scale, which is important for Ridge and Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d29b1-c6cf-4fb0-b3cc-b38efe38ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "\n",
    "for f in features:\n",
    "    raw_stats = df[f].describe()[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "    scaled_stats = combined[f + \"_scaled\"].describe()[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "\n",
    "    temp = pd.DataFrame({\n",
    "        \"Feature\": f,\n",
    "        \"Raw Mean\": raw_stats[\"mean\"],\n",
    "        \"Raw Std\": raw_stats[\"std\"],\n",
    "        \"Raw Min\": raw_stats[\"min\"],\n",
    "        \"Raw Max\": raw_stats[\"max\"],\n",
    "        \"Scaled Mean\": scaled_stats[\"mean\"],\n",
    "        \"Scaled Std\": scaled_stats[\"std\"],\n",
    "        \"Scaled Min\": scaled_stats[\"min\"],\n",
    "        \"Scaled Max\": scaled_stats[\"max\"],\n",
    "    }, index=[0])\n",
    "\n",
    "    summary = pd.concat([summary, temp], ignore_index=True)\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c0af6-02b6-446e-8eb7-3451f637455d",
   "metadata": {},
   "source": [
    "This table confirms that normalization enforces approximately zero mean and unit variance across the selected features, while also shrinking the minimum and maximum values into a narrower range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85661858-f8aa-4b65-b13e-09b8a836a6f6",
   "metadata": {},
   "source": [
    "# Baseline Model: Linear Regression\n",
    "\n",
    "### Why Linear Regression?\n",
    "- Establishes a simple baseline.\n",
    "- Tests whether linear structure exists in the dataset.\n",
    "\n",
    "### Expected Behavior\n",
    "Weak correlations and high dimensional noise should cause unregularized linear regression to underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58479c1-1db3-415e-b124-a83df51a9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse = 7.3297\n",
    "\n",
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b77538-4a34-4960-b5b4-e442054f4b1e",
   "metadata": {},
   "source": [
    "## Baseline RMSE: ~7.33\n",
    "\n",
    "As expected, Linear Regression underfits because:\n",
    "- Correlations with value_0 are weak.\n",
    "- Nonlinear patterns dominate the structure.\n",
    "- High dimensionality complicates linear modeling.\n",
    "\n",
    "This establishes our baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ad516-a9ca-4e6e-81c6-a42e21eaa02f",
   "metadata": {},
   "source": [
    "## Improved Methods\n",
    "\n",
    "Given the EDA findings of weak linear correlations, heavy-tailed feature distributions, and large differences in feature scale, we expected regularized linear models and SRP + XGBoost to outperform the plain Linear Regression baseline. The baseline model underfit due to both noise and the absence of strong individual predictors.\n",
    "\n",
    "To address these challenges, we used the following improved models:\n",
    "\n",
    "### 1. Ridge Regression  \n",
    "- Handles multicollinearity  \n",
    "- Adds L2 regularization to reduce coefficient variance  \n",
    "- Suitable when many small but meaningful signals exist across features  \n",
    "\n",
    "### 2. Lasso Regression  \n",
    "- Performs feature selection by shrinking irrelevant coefficients to zero  \n",
    "- Adds L1 regularization  \n",
    "- Well-suited for noisy high-dimensional data where only a subset of features matter  \n",
    "\n",
    "Both Ridge and Lasso hyperparameters (alpha) were tuned using cross-validation, and the best models were evaluated on the held-out test set.\n",
    "\n",
    "### 3. Sparse Random Projection (SRP)  \n",
    "- Reduces dimensionality from 316 → 50  \n",
    "- Preserves pairwise distances approximately  \n",
    "- Helps combat the curse of dimensionality while retaining structure  \n",
    "\n",
    "### 4. XGBoost (with SRP)  \n",
    "- A nonlinear boosting model capable of modeling complex interactions  \n",
    "- SRP reduces dimensionality first, allowing XGBoost to train efficiently  \n",
    "- More robust to outliers and nonlinearities than linear models  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad45504-c042-4d25-bed3-306266c08982",
   "metadata": {},
   "source": [
    "# Time-Series Forecasting\n",
    "\n",
    "To evaluate short-term electricity load forecasting, we built a separate time-series model focused on predicting the next 15-minute reading of value_0 using lag features. This complements the cross-sectional regression analysis and provides insight into temporal structure in the meter data.\n",
    "\n",
    "## Forecasting Feature Engineering\n",
    "\n",
    "We constructed a timestamp from date and time_step and generated:\n",
    "\n",
    "- Lag 1 (previous timestep)\n",
    "- Lag 24 (previous day’s same time)\n",
    "- Lag 96 (previous 4 days at same time)\n",
    "- Rolling mean (window=24)\n",
    "- Rolling standard deviation (window=24)\n",
    "\n",
    "Only rows without missing lag values were used.\n",
    "\n",
    "A chronological 80/20 split (shuffle=False) ensures training only uses past data.\n",
    "\n",
    "## Models Evaluated\n",
    "\n",
    "We tested three forecasting models:\n",
    "\n",
    "- Linear Regression  \n",
    "- Lasso Regression  \n",
    "- XGBoost Regressor  \n",
    "\n",
    "The dataset is univariate, and each model was trained on the engineered lag features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab04589-41fc-4a7d-811b-7bf48d62f15c",
   "metadata": {},
   "source": [
    "## Forecasting Performance\n",
    "\n",
    "| Model | RMSE |\n",
    "|-------|-------|\n",
    "| Linear Regression | 2.18 |\n",
    "| Lasso Regression | 2.18 |\n",
    "| XGBoost | **2.03** |\n",
    "\n",
    "XGBoost achieves the lowest RMSE, indicating nonlinear temporal patterns that linear models fail to capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c486ef-4e18-4e90-81f2-206ffaa3a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_rmse = 7.33\n",
    "lasso_rmse = 6.35\n",
    "xgb_rmse = 6.42\n",
    "\n",
    "ridge_rmse, lasso_rmse, xgb_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cec220-e897-4d3f-a47a-c3580f73f1da",
   "metadata": {},
   "source": [
    "## Why These Models?\n",
    "\n",
    "Ridge and Lasso address the high dimensionality and scale imbalance present in the dataset.  \n",
    "Among the linear models, **Lasso performs the best** because it removes irrelevant or noisy features through L1 regularization, which is effective when many of the 316 meter readings contribute weak or redundant signal.\n",
    "\n",
    "EDA revealed weak pairwise correlations and significant feature sparsity.  \n",
    "While nonlinear interactions exist, most of the measurable improvement comes from **regularization and feature selection**, not from complex tree-based modeling.\n",
    "\n",
    "We also evaluated XGBoost combined with Sparse Random Projection (SRP).  \n",
    "This pipeline introduces nonlinear decision boundaries and reduces dimensionality while preserving approximate distances.  \n",
    "Although it performs strongly, **SRP + XGBoost does not surpass Lasso**, suggesting that reducing dimensionality may discard useful structure and that the dataset benefits more from sparsity-driven feature selection than from deeper nonlinear models.\n",
    "\n",
    "In summary:  \n",
    "- **Lasso achieves the lowest RMSE (6.35)**  \n",
    "- **SRP + XGBoost is close behind (6.42)**  \n",
    "- **Ridge and baseline linear regression lag further behind**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0376b-bfda-4874-82ec-89554c3d721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Baseline\", \"Ridge\", \"Lasso\", \"XGBoost (SRP)\"]\n",
    "values = [baseline_rmse, ridge_rmse, lasso_rmse, xgb_rmse]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(models, values, color=[\"gray\",\"blue\",\"green\",\"orange\"])\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Final Model RMSE Comparison\")\n",
    "for i,v in enumerate(values):\n",
    "    plt.text(i, v+0.05, f\"{v:.2f}\", ha=\"center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96d33f-44b2-4130-9aa8-39499dc4fccc",
   "metadata": {},
   "source": [
    "## Interpretation of Final Results\n",
    "\n",
    "- Ridge performs similarly to the baseline, confirming that simple linear models cannot extract much signal from the high-dimensional meter readings.\n",
    "- Lasso provides a clear improvement by shrinking or removing noisy and redundant features, which aligns with the sparse and weakly correlated structure revealed in the EDA.\n",
    "- **Lasso achieves the best performance overall (RMSE = 6.35)**.\n",
    "- **SRP + XGBoost performs strongly (RMSE = 6.42)** but does not surpass Lasso, indicating that the dataset benefits more from aggressive feature selection than from added nonlinear complexity.\n",
    "\n",
    "These results are consistent with the EDA findings:  \n",
    "weak pairwise correlations, substantial sparsity, and heavy-tailed distributions make **regularization and noise reduction** more effective than complex nonlinear models for this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298026cd-78d7-430b-8801-41cd349c53bd",
   "metadata": {},
   "source": [
    "## Team Contributions\n",
    "\n",
    "| Name | Contribution Summary |\n",
    "|------|------------------------|\n",
    "| **Sebika Khulal** | Conducted full Exploratory Data Analysis (EDA) and implemented the baseline Linear Regression model. |\n",
    "| **Ananta Aryal** | Developed lag-based feature engineering, created the time-series split, and contributed to preprocessing. |\n",
    "| **Anubhav Bhetuwal** | Performed the normalization analysis, implemented Sparse Random Projection (SRP), tuned Ridge and Lasso models, and developed the SRP + XGBoost pipeline. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f5020-517a-4fce-9889-4cde9d1b836f",
   "metadata": {},
   "source": [
    "## Next Steps and Mitigation Plan\n",
    "\n",
    "As our models now exhibit stable behavior and we have identified the primary challenges in the dataset—sparsity, weak correlations, and scale imbalance—the following next steps outline how the project could be extended with more time and computational resources.\n",
    "\n",
    "With the core analysis completed and the behavior of both linear and nonlinear models well understood, several extensions become meaningful for a second phase of development:\n",
    "\n",
    "- Tune XGBoost more extensively (tree depth, learning rate, number of estimators) now that we have a clear performance baseline to guide parameter ranges.\n",
    "- Explore additional feature selection strategies such as stability selection or mutual information filtering, which are well suited for identifying robust predictors in high-dimensional settings.\n",
    "- Incorporate richer temporal features such as hour of day, day of week, and seasonal indicators, which become valuable after validating that the current lag-based forecasting pipeline functions reliably.\n",
    "- Evaluate probabilistic or interval prediction methods to quantify forecast uncertainty once the deterministic models are fully established.\n",
    "\n",
    "From a project management perspective:\n",
    "\n",
    "- **Sebika** would extend the EDA toward deeper temporal structure, including trend plots, seasonality diagnostics, and anomaly detection, building on insights from the initial analysis.\n",
    "- **Ananta** would expand the forecasting pipeline with more advanced lag structures and improved time-series validation after confirming the stability of the existing lag models.\n",
    "- **Anubhav** would refine model tuning using subsampling or more efficient search methods such as random search or Bayesian optimization, which are typically employed after selecting the final model families.\n",
    "\n",
    "If training time becomes a limitation during future iterations:\n",
    "\n",
    "- The number of SRP components can be adjusted to reduce dimensional overhead while preserving essential structure.\n",
    "- Smaller subsets of time steps can be used for rapid prototyping once the main modeling workflow is validated.\n",
    "- Simpler, more interpretable models such as Lasso can be prioritized when nonlinear complexity yields minimal improvements relative to the established baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027fd4b-9f6a-4c2e-bdf2-6e89fab35d2b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **OpenML Dataset #46214 — Electricity Load Diagrams 2011–2014**  \n",
    "  https://www.openml.org/d/46214\n",
    "\n",
    "- **Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python.**  \n",
    "  Journal of Machine Learning Research.  \n",
    "  https://scikit-learn.org/\n",
    "\n",
    "- **Chen & Guestrin, 2016. XGBoost: A Scalable Tree Boosting System.**  \n",
    "  Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.  \n",
    "  https://xgboost.readthedocs.io/\n",
    "\n",
    "- **Sparse Random Projection (SRP) — Scikit-learn Documentation.**  \n",
    "  https://scikit-learn.org/stable/modules/random_projection.html\n",
    "\n",
    "- **Course Materials — CS 4347: Introduction to Machine Learning.**  \n",
    "  Texas State University, Fall 2025.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
